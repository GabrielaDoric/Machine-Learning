{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sveučilište u Zagrebu  \n",
    "Fakultet elektrotehnike i računarstva  \n",
    "  \n",
    "## Strojno učenje 2019/2020  \n",
    "http://www.fer.unizg.hr/predmet/su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "### Laboratorijska vježba 2: Linearni diskriminativni modeli\n",
    "\n",
    "*Verzija: 1.3  \n",
    "Zadnji put ažurirano: 27. rujna 2019.*\n",
    "\n",
    "(c) 2015-2019 Jan Šnajder, Domagoj Alagić  \n",
    "\n",
    "Objavljeno: **30. rujna 2019.**  \n",
    "Rok za predaju: **4. studenog 2019. u 07:00h**\n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upute\n",
    "\n",
    "Prva laboratorijska vježba sastoji se od šest zadataka. U nastavku slijedite upute navedene u ćelijama s tekstom. Rješavanje vježbe svodi se na **dopunjavanje ove bilježnice**: umetanja ćelije ili više njih **ispod** teksta zadatka, pisanja odgovarajućeg kôda te evaluiranja ćelija. \n",
    "\n",
    "Osigurajte da u potpunosti **razumijete** kôd koji ste napisali. Kod predaje vježbe, morate biti u stanju na zahtjev asistenta (ili demonstratora) preinačiti i ponovno evaluirati Vaš kôd. Nadalje, morate razumjeti teorijske osnove onoga što radite, u okvirima onoga što smo obradili na predavanju. Ispod nekih zadataka možete naći i pitanja koja služe kao smjernice za bolje razumijevanje gradiva (**nemojte pisati** odgovore na pitanja u bilježnicu). Stoga se nemojte ograničiti samo na to da riješite zadatak, nego slobodno eksperimentirajte. To upravo i jest svrha ovih vježbi.\n",
    "\n",
    "Vježbe trebate raditi **samostalno**. Možete se konzultirati s drugima o načelnom načinu rješavanja, ali u konačnici morate sami odraditi vježbu. U protivnome vježba nema smisla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-617944c06de1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmlutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pylab'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlutils'"
     ]
    }
   ],
   "source": [
    "# Učitaj osnovne biblioteke...\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import mlutils\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadatci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linearna regresija kao klasifikator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U prvoj laboratorijskoj vježbi koristili smo model linearne regresije za, naravno, regresiju. Međutim, model linearne regresije može se koristiti i za **klasifikaciju**. Iako zvuči pomalo kontraintuitivno, zapravo je dosta jednostavno. Naime, cilj je naučiti funkciju $f(\\mathbf{x})$ koja za negativne primjere predviđa vrijednost $1$, dok za pozitivne primjere predviđa vrijednost $0$. U tom slučaju, funkcija $f(\\mathbf{x})=0.5$ predstavlja granicu između klasa, tj. primjeri za koje vrijedi $h(\\mathbf{x})\\geq 0.5$ klasificiraju se kao pozitivni, dok se ostali klasificiraju kao negativni.\n",
    "\n",
    "Klasifikacija pomoću linearne regresije implementirana je u razredu [`RidgeClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html). U sljedećim podzadatcima **istrenirajte** taj model na danim podatcima i **prikažite** dobivenu granicu između klasa. Pritom isključite regularizaciju ($\\alpha = 0$, odnosno `alpha=0`). Također i ispišite **točnost** vašeg klasifikacijskog modela (smijete koristiti funkciju [`metrics.accuracy_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)). Skupove podataka vizualizirajte korištenjem pomoćne funkcije ``plot_clf_problem(X, y, h=None)`` koja je dostupna u pomoćnom paketu `mlutils` (datoteku `mlutils.py` možete preuzeti sa stranice kolegija). `X` i `y` predstavljaju ulazne primjere i oznake, dok `h` predstavlja funkciju predikcije modela (npr. `model.predict`). \n",
    "\n",
    "U ovom zadatku cilj je razmotriti kako se klasifikacijski model linearne regresije ponaša na linearno odvojim i neodvojivim podatcima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Prvo, isprobajte *ugrađeni* model na linearno odvojivom skupu podataka `seven` ($N=7$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_X = np.array([[2,1], [2,3], [1,2], [3,2], [5,2], [5,4], [6,3]])\n",
    "seven_y = np.array([1, 1, 1, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "\n",
    "regression = RidgeClassifier(alpha = 0)\n",
    "regression.fit(seven_X, seven_y)\n",
    "\n",
    "weights=[]\n",
    "weights.append(regression.intercept_[0])\n",
    "weights.extend(regression.coef_.flatten())\n",
    "print (\"Težine:\\t\", weights)\n",
    "\n",
    "h=regression.predict(seven_X)\n",
    "acc = accuracy_score(seven_y, h)\n",
    "print(\"Accuracy:\\t\",acc)\n",
    "\n",
    "h = lambda x: regression.predict(x) >= 0.5\n",
    "\n",
    "\n",
    "mlutils.plot_2d_clf_problem(seven_X, seven_y, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kako bi se uvjerili da se u isprobanoj implementaciji ne radi o ničemu doli o običnoj linearnoj regresiji, napišite kôd koji dolazi do jednakog rješenja korištenjem isključivo razreda [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). Funkciju za predikciju, koju predajete kao treći argument `h` funkciji `plot_2d_clf_problem`, možete definirati lambda-izrazom: `lambda x : model.predict(x) >= 0.5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "\n",
    "linearRegression=LinearRegression()\n",
    "linearRegression.fit(seven_X,seven_y)\n",
    "\n",
    "weights = [linearRegression.intercept_]\n",
    "weights.extend(linearRegression.coef_)\n",
    "print(\"Težine: \", weights)\n",
    "\n",
    "\n",
    "h=lambda x : linearRegression.predict(x) >= 0.5\n",
    "\n",
    "mlutils.plot_2d_clf_problem(seven_X,seven_y,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako bi bila definirana granica između klasa ako bismo koristili oznake klasa $-1$ i $1$ umjesto $0$ i $1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probajte isto na linearno odvojivom skupu podataka `outlier` ($N=8$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_X = np.append(seven_X, [[12,8]], axis=0)\n",
    "outlier_y = np.append(seven_y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "\n",
    "regression = RidgeClassifier(alpha = 0)\n",
    "regression.fit(outlier_X, outlier_y)\n",
    "x_predict=regression.predict(outlier_X)\n",
    "\n",
    "\n",
    "acc = accuracy_score(outlier_y, x_predict)\n",
    "print (\"Accuracy:\\t\",acc)\n",
    "\n",
    "h = lambda x: regression.predict(x) >= 0.5\n",
    "\n",
    "mlutils.plot_2d_clf_problem(outlier_X, outlier_y, h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Zašto model ne ostvaruje potpunu točnost iako su podatci linearno odvojivi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Završno, probajte isto na linearno neodvojivom skupu podataka `unsep` ($N=8$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsep_X = np.append(seven_X, [[2,2]], axis=0)\n",
    "unsep_y = np.append(seven_y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "\n",
    "regression = RidgeClassifier(alpha = 0)\n",
    "regression.fit(unsep_X, unsep_y)\n",
    "x_predict=regression.predict(unsep_X)\n",
    "\n",
    "acc = accuracy_score(unsep_y, x_predict)\n",
    "print (\"Accuracy:\\t\",acc)\n",
    "\n",
    "h = lambda x: regression.predict(x) >= 0.5\n",
    "\n",
    "mlutils.plot_2d_clf_problem(unsep_X, unsep_y, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Očito je zašto model nije u mogućnosti postići potpunu točnost na ovom skupu podataka. Međutim, smatrate li da je problem u modelu ili u podacima? Argumentirajte svoj stav."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Višeklasna klasifikacija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postoji više načina kako se binarni klasifikatori mogu se upotrijebiti za višeklasnu klasifikaciju. Najčešće se koristi shema tzv. **jedan-naspram-ostali** (engl. *one-vs-rest*, OVR), u kojoj se trenira po jedan klasifikator $h_j$ za svaku od $K$ klasa. Svaki klasifikator $h_j$ trenira se da razdvaja primjere klase $j$ od primjera svih drugih klasa, a primjer se klasificira u klasu $j$ za koju je $h_j(\\mathbf{x})$ maksimalan.\n",
    "\n",
    "Pomoću funkcije [`datasets.make_classification`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) generirajte slučajan dvodimenzijski skup podataka od tri klase i prikažite ga koristeći funkciju `plot_2d_clf_problem`. Radi jednostavnosti, pretpostavite da nema redundantnih značajki te da je svaka od klasa \"zbijena\" upravo u jednu grupu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "\n",
    "X,y=make_classification(n_features=2,n_classes=3, n_redundant=0, n_clusters_per_class=1)\n",
    "mlutils.plot_2d_clf_problem(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenirajte tri binarna klasifikatora, $h_1$, $h_2$ i $h_3$ te prikažite granice između klasa (tri grafikona). Zatim definirajte $h(\\mathbf{x})=\\mathrm{argmax}_j h_j(\\mathbf{x})$  (napišite svoju funkciju `predict` koja to radi) i prikažite granice između klasa za taj model. Zatim se uvjerite da biste identičan rezultat dobili izravno primjenom modela `RidgeClassifier`, budući da taj model za višeklasan problem zapravo interno implementira shemu jedan-naspram-ostali.\n",
    "\n",
    "**Q:** Alternativna shema jest ona zvana **jedan-naspram-jedan** (engl, *one-vs-one*, OVO). Koja je prednost sheme OVR nad shemom OVO? A obratno?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model0, model1, model2):\n",
    "    h = lambda x: np.argmax([model0.predict(x.reshape(1,-1)), model1.predict(x.reshape(1,-1)), model2.predict(x.reshape(1,-1))])\n",
    "    return h\n",
    "\n",
    "y0 = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "for value in y:\n",
    "    if value == 0: y0.append(1);y1.append(0);y2.append(0);\n",
    "    elif value == 1:y0.append(0);y1.append(1);y2.append(0);\n",
    "    else: y0.append(0); y1.append(0);y2.append(1);\n",
    "\n",
    "plt.figure(figsize= (10, 3))\n",
    "\n",
    "classes=[y0,y1,y2]\n",
    "for i,values in enumerate(classes):\n",
    "    lin_reg = LinearRegression().fit(X, values)\n",
    "    h = lambda x: lin_reg.predict(x) >= 0.5\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.title('h'+str(i+1))\n",
    "    mlutils.plot_2d_clf_problem(X, y, h) \n",
    "\n",
    "#OVR\n",
    "plt.figure(figsize= (10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "lin_reg0= LinearRegression().fit(X, y0)\n",
    "lin_reg1= LinearRegression().fit(X, y1)\n",
    "lin_reg2= LinearRegression().fit(X, y2)\n",
    "\n",
    "h=predict(lin_reg0,lin_reg1,lin_reg2)\n",
    "plt.title('Viseklasna klasifikacija- OVR')\n",
    "mlutils.plot_2d_clf_problem(X, y, h)\n",
    "\n",
    "#Ridge Classifier\n",
    "ridge_reg = RidgeClassifier(alpha=0).fit(X, y)\n",
    "h_ridge = lambda x: ridge_reg.predict(x)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Viseklasna klasifikacija- model Ridge')\n",
    "mlutils.plot_2d_clf_problem(X, y, h_ridge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Logistička regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ovaj zadatak bavi se probabilističkim diskriminativnim modelom, **logističkom regresijom**, koja je, unatoč nazivu, klasifikacijski model.\n",
    "\n",
    "Logistička regresija tipičan je predstavnik tzv. **poopćenih linearnih modela** koji su oblika: $h(\\mathbf{x})=f(\\mathbf{w}^\\intercal\\tilde{\\mathbf{x}})$. Logistička funkcija za funkciju $f$ koristi tzv. **logističku** (sigmoidalnu) funkciju $\\sigma (x) = \\frac{1}{1 + \\textit{exp}(-x)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definirajte logističku (sigmoidalnu) funkciju $\\mathrm{sigm}(x)=\\frac{1}{1+\\exp(-\\alpha x)}$ i prikažite je za $\\alpha\\in\\{1,2,4\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+exp(-x))\n",
    "\n",
    "alfas=[1,2,4]\n",
    "\n",
    "X = linspace(-5,5)\n",
    "for alfa in alfas:\n",
    "    plt.plot(X, sigmoid(X*alfa),label=\"alfa=\"+str(alfa))\n",
    "    \n",
    "legend();\n",
    "grid();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Zašto je sigmoidalna funkcija prikladan izbor za aktivacijsku funkciju poopćenoga linearnog modela? \n",
    "</br>\n",
    "\n",
    "**Q**: Kakav utjecaj ima faktor $\\alpha$ na oblik sigmoide? Što to znači za model logističke regresije (tj. kako izlaz modela ovisi o normi vektora težina $\\mathbf{w}$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementirajte funkciju \n",
    "\n",
    "> `lr_train(X, y, eta=0.01, max_iter=2000, alpha=0, epsilon=0.0001, trace=False)` \n",
    "\n",
    "za treniranje modela logističke regresije gradijentnim spustom (*batch* izvedba). Funkcija uzima označeni skup primjera za učenje (matrica primjera `X` i vektor oznaka `y`) te vraća $(n+1)$-dimenzijski vektor težina tipa `ndarray`. Ako je `trace=True`, funkcija dodatno vraća listu (ili matricu) vektora težina $\\mathbf{w}^0,\\mathbf{w}^1,\\dots,\\mathbf{w}^k$ generiranih kroz sve iteracije optimizacije, od 0 do $k$. Optimizaciju treba provoditi dok se ne dosegne `max_iter` iteracija, ili kada razlika u pogrešci unakrsne entropije između dviju iteracija padne ispod vrijednosti `epsilon`. Parametar `alpha` predstavlja faktor L2-regularizacije.\n",
    "\n",
    "Preporučamo definiranje pomoćne funkcije `lr_h(x,w)` koja daje predikciju za primjer `x` uz zadane težine `w`. Također, preporučamo i funkciju `cross_entropy_error(X,y,w)` koja izračunava pogrešku unakrsne entropije modela na označenom skupu `(X,y)` uz te iste težine.\n",
    "\n",
    "**NB:** Obratite pozornost na to da je način kako su definirane oznake ($\\{+1,-1\\}$ ili $\\{1,0\\}$) kompatibilan s izračunom funkcije gubitka u optimizacijskome algoritmu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def lr_h(X,w, fit_intercept=False):\n",
    "    h = sigmoid(np.dot(X, w[1:]) + w[0]) if fit_intercept else sigmoid(np.dot(X, w))\n",
    "    return h\n",
    "\n",
    "\n",
    "def cross_entropy_error(X, y, w, fit_intercept=False):  \n",
    "    h = lr_h(X, w, fit_intercept=fit_intercept)\n",
    "    error=0\n",
    "    error = -y*np.log(h)-(1-y)*np.log(1-h)    \n",
    "    error = error.sum()/(X.shape[0])\n",
    "    return error\n",
    "\n",
    "\n",
    "def lr_train(X, y, eta=0.01, max_iter=2000, alpha=0, epsilon=0.0001, trace=False):\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    one = np.ones((m, 1))\n",
    "    X = np.hstack((one, X))\n",
    "   \n",
    "\n",
    "    w = np.array([0]*(n+1), dtype=float)\n",
    "    weights_list = []\n",
    "    \n",
    "    weights_list.append(w.copy())\n",
    "    \n",
    "    previous_error = cross_entropy_error(X, y, w)\n",
    "    counter=1\n",
    "    while counter<max_iter:\n",
    "        \n",
    "        h = lr_h(X, w)\n",
    "        delta_w = np.dot(h-y, X)\n",
    "\n",
    "        for i,weights in enumerate(w):\n",
    "            if (i==0):\n",
    "                w[i]=w[i] - eta*delta_w[i]\n",
    "            else:\n",
    "                w[i]=w[i]*(1 - eta*alpha) - eta*delta_w[i] \n",
    "                \n",
    "        weights_list.append(w.copy())\n",
    "\n",
    "            \n",
    "        current_error = cross_entropy_error(X, y, w)\n",
    "        if previous_error-current_error < epsilon:\n",
    "            break\n",
    "        previous_error = current_error\n",
    "        counter+=1   \n",
    "    if trace:\n",
    "        return w, weights_list\n",
    "    else:\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koristeći funkciju `lr_train`, trenirajte model logističke regresije na skupu `seven`, prikažite dobivenu granicu između klasa te  izračunajte pogrešku unakrsne entropije. \n",
    "\n",
    "**NB:** Pripazite da modelu date dovoljan broj iteracija."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_X = np.array([[2,1], [2,3], [1,2], [3,2], [5,2], [5,4], [6,3]])\n",
    "seven_y = np.array([1, 1, 1, 1, 0, 0, 0])\n",
    "\n",
    "weights = lr_train(seven_X, seven_y)\n",
    "\n",
    "error=cross_entropy_error(seven_X, seven_y, weights,fit_intercept=True)\n",
    "print(\"Cross-entropy error:\\t\",error)\n",
    "\n",
    "h=lambda x : lr_h(x, weights, fit_intercept=True) >= 0.5\n",
    "mlutils.plot_2d_clf_problem(seven_X, seven_y,h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Koji kriterij zaustavljanja je aktiviran?\n",
    "\n",
    "**Q:** Zašto dobivena pogreška unakrsne entropije nije jednaka nuli?\n",
    "\n",
    "**Q:** Kako biste utvrdili da je optimizacijski postupak doista pronašao hipotezu koja minimizira pogrešku učenja? O čemu to ovisi?\n",
    "\n",
    "**Q:** Na koji način biste preinačili kôd ako biste htjeli da se optimizacija izvodi stohastičkim gradijentnim spustom (*online learning*)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prikažite na jednom grafikonu pogrešku unakrsne entropije (očekivanje logističkog gubitka) i pogrešku klasifikacije (očekivanje gubitka 0-1) na skupu `seven` kroz iteracije optimizacijskog postupka. Koristite trag težina funkcije `lr_train` iz zadatka (b) (opcija `trace=True`). Na drugom grafikonu prikažite pogrešku unakrsne entropije kao funkciju broja iteracija za različite stope učenja, $\\eta\\in\\{0.005,0.01,0.05,0.1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "best_weights, weights_trace = lr_train(seven_X, seven_y, trace=True)\n",
    "\n",
    "\n",
    "cross_error = []\n",
    "classification_error = []\n",
    "\n",
    "for i in range(len(weights_trace)):\n",
    "    cross_error.append(cross_entropy_error(seven_X, seven_y, weights_trace[i],fit_intercept=True))\n",
    "    \n",
    "    h = lr_h(seven_X, weights_trace[i], fit_intercept=True)\n",
    "    classification_error.append(zero_one_loss(\n",
    "        seven_y, [1 if h[j] >= 0.5 else 0 for j in range(len(h))]))\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(10,5))\n",
    "subplot(1,2,1)    \n",
    "plt.plot( cross_error, label='Cross-entropy error')\n",
    "plt.plot( classification_error, label='Classification error')\n",
    "\n",
    "\n",
    "#razlicite stope ucenja\n",
    "learning_rates=[0.005,0.01,0.05,0.1]\n",
    "for learning_rate in learning_rates:\n",
    "    best_weights, weights_trace = lr_train(seven_X, seven_y, eta=learning_rate, trace=True)\n",
    "    \n",
    "    cross_error = []\n",
    "    for i in range(len(weights_trace)):\n",
    "        cross_error.append(cross_entropy_error(seven_X, seven_y, weights_trace[i],fit_intercept=True))\n",
    "    \n",
    "    subplot(1,2,2)    \n",
    "    plt.plot(cross_error, label='eta='+str(learning_rate))\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q:** Zašto je pogreška unakrsne entropije veća od pogreške klasifikacije? Je li to uvijek slučaj kod logističke regresije i zašto?\n",
    "\n",
    "**Q:** Koju stopu učenja $\\eta$ biste odabrali i zašto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upoznajte se s klasom [`linear_model.LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) koja implementira logističku regresiju. Usporedite rezultat modela na skupu `seven` s rezultatom koji dobivate pomoću vlastite implementacije algoritma.\n",
    "\n",
    "**NB:** Kako ugrađena implementacija koristi naprednije verzije optimizacije funkcije, vrlo je vjerojatno da Vam se rješenja neće poklapati, ali generalne performanse modela bi trebale. Ponovno, pripazite na broj iteracija i snagu regularizacije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logisticRegression = LogisticRegression(C=5).fit(seven_X, seven_y)\n",
    "\n",
    "h=lambda x : logisticRegression.predict(x) \n",
    "\n",
    "mlutils.plot_2d_clf_problem(seven_X, seven_y, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analiza logističke regresije"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koristeći ugrađenu implementaciju logističke regresije, provjerite kako se logistička regresija nosi s vrijednostima koje odskaču. Iskoristite skup `outlier` iz prvog zadatka. Prikažite granicu između klasa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Zašto se rezultat razlikuje od onog koji je dobio model klasifikacije linearnom regresijom iz prvog zadatka?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "\n",
    "lr_model = LogisticRegression(C=5).fit(outlier_X, outlier_y)\n",
    "acc=accuracy_score(outlier_y, lr_model.predict(outlier_X))\n",
    "print(\"Accuracy:\\t\"+str(acc))\n",
    "h=lambda x : lr_model.predict(x) >= 0.5\n",
    "mlutils.plot_2d_clf_problem(outlier_X, outlier_y, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenirajte model logističke regresije na skupu `seven` te na dva odvojena grafikona prikažite, kroz iteracije optimizacijskoga algoritma, (1) izlaz modela $h(\\mathbf{x})$ za svih sedam primjera te (2) vrijednosti težina $w_0$, $w_1$, $w_2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "\n",
    "best_weights, weights_trace = lr_train(seven_X, seven_y, trace=True)\n",
    "\n",
    "predictions=[]\n",
    "for i in range(0,seven_X.shape[0]):\n",
    "    predictions.append([])\n",
    "\n",
    "#predictions\n",
    "for i in range(0, len(weights_trace)):\n",
    "    prediction = lr_h(seven_X, weights_trace[i],fit_intercept=True)\n",
    "    for j in range(0, np.shape(prediction)[0]):\n",
    "        predictions[j].append(prediction[j])\n",
    "\n",
    "plt.figure(figsize=(13, 6))    \n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(seven_X.shape[0]):\n",
    "    plt.plot(predictions[i], label='Pr.'+str(i+1))\n",
    "plt.legend();        \n",
    "        \n",
    "#weights\n",
    "w0 = []\n",
    "w1 = []\n",
    "w2 = []\n",
    "for i in range(len(weights_trace)):\n",
    "    w0.append(weights_trace[i][0])\n",
    "    w1.append(weights_trace[i][1])\n",
    "    w2.append(weights_trace[i][2])\n",
    "    \n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "W=[w0,w1,w2]\n",
    "for i,w in enumerate(W):\n",
    "    plt.plot(w, label='w'+str(i+1)) \n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponovite eksperiment iz podzadatka (b) koristeći linearno neodvojiv skup podataka `unsep` iz prvog zadatka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Usporedite grafikone za slučaj linearno odvojivih i linearno neodvojivih primjera te komentirajte razliku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "\n",
    "\n",
    "best_weights, weights_trace = lr_train(unsep_X, unsep_y, trace=True)\n",
    "\n",
    "predictions=[]\n",
    "for i in range(0,unsep_X.shape[0]):\n",
    "    predictions.append([])\n",
    "\n",
    "#predictions\n",
    "for i in range(0, len(weights_trace)):\n",
    "    prediction = lr_h(unsep_X, weights_trace[i],fit_intercept=True)\n",
    "    for j in range(0, np.shape(prediction)[0]):\n",
    "        predictions[j].append(prediction[j])\n",
    "\n",
    "plt.figure(figsize=(13, 6))    \n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(unsep_X.shape[0]):\n",
    "    plt.plot(predictions[i], label='Pr.'+str(i+1))\n",
    "plt.legend();        \n",
    "        \n",
    "#weights\n",
    "w0 = []\n",
    "w1 = []\n",
    "w2 = []\n",
    "for i in range(len(weights_trace)):\n",
    "    w0.append(weights_trace[i][0])\n",
    "    w1.append(weights_trace[i][1])\n",
    "    w2.append(weights_trace[i][2])\n",
    "    \n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "W=[w0,w1,w2]\n",
    "for i,w in enumerate(W):\n",
    "    plt.plot(w, label='w'+str(i+1)) \n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Regularizirana logistička regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenirajte model logističke regresije na skupu `seven` s različitim faktorima L2-regularizacije, $\\alpha\\in\\{0,1,10,100\\}$. Prikažite na dva odvojena grafikona (1) pogrešku unakrsne entropije te (2) L2-normu vektora $\\mathbf{w}$ kroz iteracije optimizacijskog algoritma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Jesu li izgledi krivulja očekivani i zašto?\n",
    "\n",
    "**Q:** Koju biste vrijednost za $\\alpha$ odabrali i zašto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "alphas=[0, 1, 10, 100]\n",
    "for alpha in alphas:\n",
    "    best_weights, weight_trace = lr_train(seven_X, seven_y, alpha=alpha, trace=True)\n",
    "\n",
    "    cross_error = []\n",
    "    L2_norm = []\n",
    "    for weights in weight_trace:\n",
    "        cross_error.append(cross_entropy_error(seven_X, seven_y, weights,fit_intercept=True))\n",
    "        L2_norm.append(linalg.norm(weights))\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(cross_error, label='Cross-entropy error, alfa='+str(alpha))\n",
    "    plt.legend();\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(L2_norm, label='L2-norm, alfa='+str(alpha))\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Logistička regresija s funkcijom preslikavanja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proučite funkciju [`datasets.make_classification`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html). Generirajte i prikažite dvoklasan skup podataka s ukupno $N=100$ dvodimenzijskih ($n=2)$ primjera, i to sa dvije grupe po klasi (`n_clusters_per_class=2`). Malo je izgledno da će tako generiran skup biti linearno odvojiv, međutim to nije problem jer primjere možemo preslikati u višedimenzijski prostor značajki pomoću klase [`preprocessing.PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html), kao što smo to učinili kod linearne regresije u prvoj laboratorijskoj vježbi. Trenirajte model logističke regresije koristeći za preslikavanje u prostor značajki polinomijalnu funkciju stupnja $d=2$ i stupnja $d=3$. Prikažite dobivene granice između klasa. Možete koristiti svoju implementaciju, ali se radi brzine preporuča koristiti `linear_model.LogisticRegression`. Regularizacijski faktor odaberite po želji.\n",
    "\n",
    "**NB:** Kao i ranije, za prikaz granice između klasa koristite funkciju `plot_2d_clf_problem`. Funkciji kao argumente predajte izvorni skup podataka, a preslikavanje u prostor značajki napravite unutar poziva funkcije `h` koja čini predikciju, na sljedeći način:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#poly = PolynomialFeatures(2)\n",
    "#...\n",
    "#mlutils.plot_2d_clf_problem(X, y, lambda x : model.predict(poly.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_classes=2, n_clusters_per_class=2)  \n",
    "mlutils.plot_2d_clf_problem(X, y)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "degrees=[2,3]\n",
    "\n",
    "\n",
    "for i,degree in enumerate(degrees):\n",
    "    subplot(1,2,i+1)\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    new_X = poly.fit_transform(X)\n",
    "    lr_model = LogisticRegression().fit(new_X, y)\n",
    "    h = lambda x : lr_model.predict(poly.transform(x))\n",
    "    mlutils.plot_2d_clf_problem(X, y, h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Koji biste stupanj polinoma upotrijebili i zašto? Je li taj odabir povezan s odabirom regularizacijskog faktora $\\alpha$? Zašto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
