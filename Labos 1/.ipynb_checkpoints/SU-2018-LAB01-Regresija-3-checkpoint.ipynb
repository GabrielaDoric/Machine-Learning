{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sveučilište u Zagrebu  \n",
    "Fakultet elektrotehnike i računarstva  \n",
    "  \n",
    "## Strojno učenje 2018/2019  \n",
    "http://www.fer.unizg.hr/predmet/su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "### Laboratorijska vježba 1: Regresija\n",
    "\n",
    "*Verzija: 1.1  \n",
    "Zadnji put ažurirano: 12. listopada 2018.*\n",
    "\n",
    "(c) 2015-2018 Jan Šnajder, Domagoj Alagić, Mladen Karan \n",
    "\n",
    "Objavljeno: **12. listopada 2018.**  \n",
    "Rok za predaju: **22. listopada 2018. u 07:00h**\n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upute\n",
    "\n",
    "Prva laboratorijska vježba sastoji se od osam zadataka. U nastavku slijedite upute navedene u ćelijama s tekstom. Rješavanje vježbe svodi se na **dopunjavanje ove bilježnice**: umetanja ćelije ili više njih **ispod** teksta zadatka, pisanja odgovarajućeg kôda te evaluiranja ćelija. \n",
    "\n",
    "Osigurajte da u potpunosti **razumijete** kôd koji ste napisali. Kod predaje vježbe, morate biti u stanju na zahtjev asistenta (ili demonstratora) preinačiti i ponovno evaluirati Vaš kôd. Nadalje, morate razumjeti teorijske osnove onoga što radite, u okvirima onoga što smo obradili na predavanju. Ispod nekih zadataka možete naći i pitanja koja služe kao smjernice za bolje razumijevanje gradiva (**nemojte pisati** odgovore na pitanja u bilježnicu). Stoga se nemojte ograničiti samo na to da riješite zadatak, nego slobodno eksperimentirajte. To upravo i jest svrha ovih vježbi.\n",
    "\n",
    "Vježbe trebate raditi **samostalno**. Možete se konzultirati s drugima o načelnom načinu rješavanja, ali u konačnici morate sami odraditi vježbu. U protivnome vježba nema smisla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Učitaj osnovne biblioteke...\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadatci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Jednostavna regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadan je skup primjera $\\mathcal{D}=\\{(x^{(i)},y^{(i)})\\}_{i=1}^4 = \\{(0,4),(1,1),(2,2),(4,5)\\}$. Primjere predstavite matrixom $\\mathbf{X}$ dimenzija $N\\times n$ (u ovom slučaju $4\\times 1$) i vektorom oznaka $\\textbf{y}$, dimenzija $N\\times 1$ (u ovom slučaju $4\\times 1$), na sljedeći način:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0],[1],[2],[4]])\n",
    "y = np.array([4,1,2,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Proučite funkciju [`PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) iz biblioteke `sklearn` i upotrijebite je za generiranje matrice dizajna $\\mathbf{\\Phi}$ koja ne koristi preslikavanje u prostor više dimenzije (samo će svakom primjeru biti dodane *dummy* jedinice; $m=n+1$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "from sklearn.preprocessing import PolynomialFeatures as pf\n",
    "design_matrix = pf(1, False, True).fit_transform(X)\n",
    "print(design_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upoznajte se s modulom [`linalg`](http://docs.scipy.org/doc/numpy/reference/routines.linalg.html). Izračunajte težine $\\mathbf{w}$ modela linearne regresije kao $\\mathbf{w}=(\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi})^{-1}\\mathbf{\\Phi}^\\intercal\\mathbf{y}$. Zatim se uvjerite da isti rezultat možete dobiti izračunom pseudoinverza $\\mathbf{\\Phi}^+$ matrice dizajna, tj. $\\mathbf{w}=\\mathbf{\\Phi}^+\\mathbf{y}$, korištenjem funkcije [`pinv`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "print('Prvi nacin:')\n",
    "w1 = np.dot(np.dot(lin.inv(np.dot(np.transpose(design_matrix),design_matrix)), np.transpose(design_matrix)),y)\n",
    "print('w1 je:' , w1)\n",
    "\n",
    "print('Drugi nacin:')\n",
    "w2 = np.dot(lin.pinv(design_matrix),y)\n",
    "print('w2 je:' , w2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radi jasnoće, u nastavku je vektor $\\mathbf{x}$ s dodanom *dummy* jedinicom $x_0=1$ označen kao $\\tilde{\\mathbf{x}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prikažite primjere iz $\\mathcal{D}$ i funkciju $h(\\tilde{\\mathbf{x}})=\\mathbf{w}^\\intercal\\tilde{\\mathbf{x}}$. Izračunajte pogrešku učenja prema izrazu $E(h|\\mathcal{D})=\\frac{1}{2}\\sum_{i=1}^N(\\tilde{\\mathbf{y}}^{(i)} - h(\\tilde{\\mathbf{x}}))^2$. Možete koristiti funkciju srednje kvadratne pogreške [`mean_squared_error`]( http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) iz modula [`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).\n",
    "\n",
    "**Q:** Gore definirana funkcija pogreške $E(h|\\mathcal{D})$ i funkcija srednje kvadratne pogreške nisu posve identične. U čemu je razlika? Koja je \"realnija\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "print design_matrix\n",
    "print w1\n",
    "h=np.dot(design_matrix, w1)\n",
    "print(h)\n",
    "error=MSE(y, h)\n",
    "print('Pogreska ucenja je:', error)\n",
    "\n",
    "plt.plot(X, y, 's', X, h)\n",
    "plt.legend(['Primjeri iz D', 'Funkcija h(x)'], loc='upper right')\n",
    "plt.axis([-1, 5, 0, 10]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uvjerite se da za primjere iz $\\mathcal{D}$ težine $\\mathbf{w}$ ne možemo naći rješavanjem sustava $\\mathbf{w}=\\mathbf{\\Phi}^{-1}\\mathbf{y}$, već da nam doista treba pseudoinverz.\n",
    "\n",
    "**Q:** Zašto je to slučaj? Bi li se problem mogao riješiti preslikavanjem primjera u višu dimenziju? Ako da, bi li to uvijek funkcioniralo, neovisno o skupu primjera $\\mathcal{D}$? Pokažite na primjeru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "try:\n",
    "    np.dot(lin.inv(design_matrix),y)\n",
    "except:\n",
    "    print('Zbilja je potreban psudoinverz za izračun težina')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proučite klasu [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) iz modula [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model). Uvjerite se da su težine koje izračunava ta funkcija (dostupne pomoću atributa `coef_` i `intercept_`) jednake onima koje ste izračunali gore. Izračunajte predikcije modela (metoda `predict`) i uvjerite se da je pogreška učenja identična onoj koju ste ranije izračunali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as lr\n",
    "\n",
    "LinR = lr().fit(design_matrix,y)\n",
    "w1_lin = LinR.coef_[1]\n",
    "w0_lin = LinR.intercept_\n",
    "w = [w0_lin, w1_lin]\n",
    "print('Nove tezine su:', w)\n",
    "\n",
    "h_lin = LinR.predict(design_matrix)\n",
    "print('Nova predikcija je:', h)\n",
    "\n",
    "error2 = MSE(y, h_lin)\n",
    "print('Prva pogreska je:', error)\n",
    "print('Druga pogreska je:', error2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Polinomijalna regresija i utjecaj šuma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Razmotrimo sada regresiju na većem broju primjera. Koristite funkciju `make_labels(X, f, noise=0)` koja uzima matricu neoznačenih primjera $\\mathbf{X}_{N\\times n}$ te generira vektor njihovih oznaka $\\mathbf{y}_{N\\times 1}$. Oznake se generiraju kao $y^{(i)} = f(x^{(i)})+\\mathcal{N}(0,\\sigma^2)$, gdje je $f:\\mathbb{R}^n\\to\\mathbb{R}$ stvarna funkcija koja je generirala podatke (koja nam je u stvarnosti nepoznata), a $\\sigma$ je standardna devijacija Gaussovog šuma, definirana parametrom `noise`. Za generiranje šuma koristi se funkcija [`numpy.random.normal`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html). \n",
    "\n",
    "Generirajte skup za učenje od $N=50$ primjera uniformno distribuiranih u intervalu $[-5,5]$ pomoću funkcije $f(x) = 5 + x -2 x^2 -5 x^3$ uz šum  $\\sigma=200$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal\n",
    "def make_labels(X, f, noise=0) :\n",
    "    return map(lambda x : f(x) + (normal(0,noise) if noise>0 else 0), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_instances(x1, x2, N) :\n",
    "    return np.array([np.array([x]) for x in np.linspace(x1,x2,N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 200\n",
    "N = 50\n",
    "f = lambda x: (5 + x - 2*(x**2) - 5*(x**3))\n",
    "x = np.linspace(-5,5,N)\n",
    "y_i = make_labels(x, f, noise)\n",
    "y_i = np.array(list(y_i))\n",
    "x_6 = x\n",
    "y_6 = y_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prikažite taj skup funkcijom [`scatter`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, f(x), 'r', label = 'Funkcija f(x)')\n",
    "plt.scatter(x, y_i, label = 'Generirani skup za ucenje')\n",
    "plt.legend(loc = 'upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenirajte model polinomijalne regresije stupnja $d=3$. Na istom grafikonu prikažite naučeni model $h(\\mathbf{x})=\\mathbf{w}^\\intercal\\tilde{\\mathbf{x}}$ i primjere za učenje. Izračunajte pogrešku učenja modela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "from sklearn.preprocessing import PolynomialFeatures as pf\n",
    "\n",
    "design_matrix_3 = pf(3).fit_transform(x.reshape(-1,1))\n",
    "w_3 = np.dot(lin.pinv(design_matrix_3),y_i)\n",
    "h_3 = np.dot(design_matrix_3, w_3)\n",
    "error_3 = MSE(y_i, h_3)\n",
    "\n",
    "print('Pogreska treniranog modela d=3 je: ',error_3, '\\n')\n",
    "\n",
    "plt.scatter(x.reshape(-1,1), y_i, label = 'Primjeri za ucenje');\n",
    "plt.plot(x, h_3, 'r', linewidth = 2, label = 'Nauceni model h(x)');\n",
    "plt.legend(loc = 'upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Odabir modela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Na skupu podataka iz zadatka 2 trenirajte pet modela linearne regresije $\\mathcal{H}_d$ različite složenosti, gdje je $d$ stupanj polinoma, $d\\in\\{1,3,5,10,20\\}$. Prikažite na istome grafikonu skup za učenje i funkcije $h_d(\\mathbf{x})$ za svih pet modela (preporučujemo koristiti `plot` unutar `for` petlje). Izračunajte pogrešku učenja svakog od modela.\n",
    "\n",
    "**Q:** Koji model ima najmanju pogrešku učenja i zašto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "\n",
    "d = [1, 3, 5, 10, 20]\n",
    "errors = []\n",
    "h_s = []\n",
    "w_s = []\n",
    "design_matrixs = []\n",
    "\n",
    "for i in d:\n",
    "    design_matrixs.append(pf(i).fit_transform(x.reshape(-1,1)))\n",
    "for i in range(0,len(d)):\n",
    "    w_s.insert(i, np.dot(lin.pinv(design_matrixs[i]),y_i))\n",
    "    h_s.insert(i, np.dot(design_matrixs[i], w_s[i]))\n",
    "#errors\n",
    "for i in range(0,len(d)):\n",
    "    errors.insert(i, MSE(y_i, h_s[i]))\n",
    "    print('Pogreska ucenja za d = %d je: '% d[i],errors[i])\n",
    "\n",
    "for i in range(0,len(d)):\n",
    "    plt.figure()\n",
    "    plt.scatter(x.reshape(-1,1), y_i, label = 'Skup za ucenje');\n",
    "    plt.plot(x, h_s[i], 'r', linewidth = 2, label = 'Nauceni model hd(x)');\n",
    "    plt.legend(loc = 'upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Razdvojite skup primjera iz zadatka 2 pomoću funkcije [`cross_validation.train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split) na skup za učenja i skup za ispitivanje u omjeru 1:1. Prikažite na jednom grafikonu pogrešku učenja i ispitnu pogrešku za modele polinomijalne regresije $\\mathcal{H}_d$, sa stupnjem polinoma $d$ u rasponu $d\\in [1,2,\\ldots,20]$. Radi preciznosti, funkcije $h(\\mathbf{x})$ iscrtajte na cijelom skupu primjera (ali pogrešku generalizacije računajte, naravno, samo na ispitnome skupu). Budući da kvadratna pogreška brzo raste za veće stupnjeve polinoma, umjesto da iscrtate izravno iznose pogrešaka, iscrtajte njihove logaritme.\n",
    "\n",
    "**NB:** Podjela na skupa za učenje i skup za ispitivanje mora za svih pet modela biti identična.\n",
    "\n",
    "**Q:** Je li rezultat u skladu s očekivanjima? Koji biste model odabrali i zašto?\n",
    "\n",
    "**Q:** Pokrenite iscrtavanje više puta. U čemu je problem? Bi li problem bio jednako izražen kad bismo imali više primjera? Zašto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "x_train,x_test,y_train,y_test = tts(x, y_i, test_size=0.5)\n",
    "error_train = []\n",
    "error_test = []\n",
    "d = range(1,20)\n",
    "\n",
    "for i in d:\n",
    "    design_matrix_train = pf(i).fit_transform(x_train.reshape(-1,1))\n",
    "    design_matrix_test = pf(i).fit_transform(x_test.reshape(-1,1))\n",
    "    w_train = np.dot(lin.pinv(design_matrix_train),y_train)\n",
    "    h_train = np.dot(design_matrix_train, w_train)\n",
    "    h_test = np.dot(design_matrix_test, w_train)\n",
    "    error_train.insert(i, np.log(MSE(y_train, h_train)))\n",
    "    error_test.insert(i, np.log(MSE(y_test, h_test)))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(d, error_train, d, error_test)\n",
    "plt.grid()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Greska ucenja', 'Ispitna greska'], loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Točnost modela ovisi o (1) njegovoj složenosti (stupanj $d$ polinoma), (2) broju primjera $N$, i (3) količini šuma. Kako biste to analizirali, nacrtajte grafikone pogrešaka kao u 3b, ali za sve kombinacija broja primjera $N\\in\\{100,200,1000\\}$ i količine šuma $\\sigma\\in\\{100,200,500\\}$ (ukupno 9 grafikona). Upotrijebite funkciju [`subplots`](http://matplotlib.org/examples/pylab_examples/subplots_demo.html) kako biste pregledno posložili grafikone u tablicu $3\\times 3$. Podatci se generiraju na isti način kao u zadatku 2.\n",
    "\n",
    "**NB:** Pobrinite se da svi grafikoni budu generirani nad usporedivim skupovima podataka, na sljedeći način. Generirajte najprije svih 1000 primjera, podijelite ih na skupove za učenje i skupove za ispitivanje (dva skupa od po 500 primjera). Zatim i od skupa za učenje i od skupa za ispitivanje načinite tri različite verzije, svaka s drugačijom količinom šuma (ukupno 2x3=6 verzija podataka). Kako bi simulirali veličinu skupa podataka, od tih dobivenih 6 skupova podataka uzorkujte trećinu, dvije trećine i sve podatke. Time ste dobili 18 skupova podataka -- skup za učenje i za testiranje za svaki od devet grafova."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q:*** Jesu li rezultati očekivani? Obrazložite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "def errors(X_train, X_test, y_train, y_test):\n",
    "    error_train = []\n",
    "    error_test = []\n",
    "    for d in range(1,21):\n",
    "        design_matrix_train = pf(i).fit_transform(X_train.reshape(-1,1))\n",
    "        design_matrix_test = pf(i).fit_transform(X_test.reshape(-1,1))\n",
    "        w_train = np.dot(lin.pinv(design_matrix_train),y_train)\n",
    "        h_train = np.dot(design_matrix_train, w_train)\n",
    "        h_test = np.dot(design_matrix_test, w_train)\n",
    "        error_train.insert(i, np.log(MSE(y_train, h_train)))\n",
    "        error_test.insert(i, np.log(MSE(y_test, h_test)))\n",
    "    return error_train, error_test\n",
    "\n",
    "NN = [50, 100, 500]\n",
    "sigma2 = [100, 200, 500]\n",
    "error_train_2 = []\n",
    "error_test_2 = []\n",
    "x_proba = make_instances(-5,5,1000)\n",
    "X_train, X_test = tts(x_proba, test_size=0.5)\n",
    "#print X_train\n",
    "#print X_test\n",
    "fig, axs = plt.subplots(3,3,figsize=(15,15))\n",
    "axs = axs.ravel()\n",
    "d = [i for i in range(1, 21)]\n",
    "\n",
    "for i, sig in enumerate(sigma2):\n",
    "    y_train_proba = np.array(list(make_labels(X_train, f, sig)))\n",
    "    y_test_proba = np.array(list(make_labels(X_test, f, sig)))\n",
    "    #axs[i*3+3].scatter(X_train, y_train_proba)\n",
    "    #axs[i*3+3].scatter(X_test, y_test_proba)\n",
    "    for j, nn in enumerate(NN):\n",
    "        index = np.random.choice(np.arange(500), nn, replace=False)\n",
    "        X1_train = X_train[index]\n",
    "        y1_train = y_train_proba[index]\n",
    "        index = np.random.choice(np.arange(500), nn, replace=False)\n",
    "        X1_test = X_test[index]\n",
    "        y1_test = y_test_proba[index]\n",
    "        train, test = errors(X1_train, X1_test, y1_train, y1_test)\n",
    "        \n",
    "        axs[i*3+j].set_title(\"N = \" + str(nn) + \", sigma = \" + str(sig))\n",
    "        #print train\n",
    "        #print test\n",
    "        axs[i*3+j].plot(d, train, label=\"TRAIN\")\n",
    "        axs[i*3+j].plot(d, test, label=\"TEST\")\n",
    "        axs[i*3+j].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Regularizirana regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "U gornjim eksperimentima nismo koristili **regularizaciju**. Vratimo se najprije na primjer iz zadatka 1. Na primjerima iz tog zadatka izračunajte težine $\\mathbf{w}$ za polinomijalni regresijski model stupnja $d=3$ uz L2-regularizaciju (tzv. *ridge regression*), prema izrazu $\\mathbf{w}=(\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi}+\\lambda\\mathbf{I})^{-1}\\mathbf{\\Phi}^\\intercal\\mathbf{y}$. Napravite izračun težina za regularizacijske faktore $\\lambda=0$, $\\lambda=1$ i $\\lambda=10$ te usporedite dobivene težine.\n",
    "\n",
    "**Q:** Kojih je dimenzija matrica koju treba invertirati?\n",
    "\n",
    "**Q:** Po čemu se razlikuju dobivene težine i je li ta razlika očekivana? Obrazložite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vaš kôd ovdje\n",
    "lamda = [0,1,10]\n",
    "design_matrix_4 = pf(3).fit_transform(X)\n",
    "w_L2 = []\n",
    "\n",
    "w_regularizacija = lambda l: np.dot(np.dot(np.linalg.inv(np.dot(design_matrix_4.T, design_matrix_4) + np.dot(l, np.eye(4))), design_matrix_4.T), y)\n",
    "\n",
    "for i in range(0,3):\n",
    "    w_L2.insert(i, w_regularizacija(lamda[i]))\n",
    "    #print w_L2[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proučite klasu [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) iz modula [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model), koja implementira L2-regularizirani regresijski model. Parametar $\\alpha$ odgovara parametru $\\lambda$. Primijenite model na istim primjerima kao u prethodnom zadatku i ispišite težine $\\mathbf{w}$ (atributi `coef_` i `intercept_`).\n",
    "\n",
    "**Q:** Jesu li težine identične onima iz zadatka 4a? Ako nisu, objasnite zašto je to tako i kako biste to popravili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "#popravi weights\n",
    "for i in range(0,3):\n",
    "    w_L2_2 = Ridge(alpha=lamda[i]).fit(design_matrix_4, y)\n",
    "    wo_L2 = w_L2_2.intercept_\n",
    "    weights = []\n",
    "    for j in range(0, len(w_L2_2.coef_[1:])):\n",
    "        weights.append(w_L2_2.coef_[j])\n",
    "    print wo_L2\n",
    "    print weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Regularizirana polinomijalna regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Vratimo se na slučaj $N=50$ slučajno generiranih primjera iz zadatka 2. Trenirajte modele polinomijalne regresije $\\mathcal{H}_{\\lambda,d}$ za $\\lambda\\in\\{0,100\\}$ i $d\\in\\{2,10\\}$ (ukupno četiri modela). Skicirajte pripadne funkcije $h(\\mathbf{x})$ i primjere (na jednom grafikonu; preporučujemo koristiti `plot` unutar `for` petlje).\n",
    "\n",
    "**Q:** Jesu li rezultati očekivani? Obrazložite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "lamda_5 = [0,100]\n",
    "d = [2,10]\n",
    "N = 50\n",
    "noise=200\n",
    "f = lambda x: (5 + x - 2*(x**2) - 5*(x**3))\n",
    "x_5 = make_instances(-5,5,N)\n",
    "yi_5 = make_labels(x_5, f, noise)\n",
    "yi_5 = np.array(list(yi_5))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x_5, yi_5, label=\"Dots\");\n",
    "for i in range(0,2):\n",
    "    for j in range(0,2):\n",
    "        design_matrix_5 = pf(d[i]).fit_transform(x_5.reshape(-1,1))\n",
    "        r = Ridge(alpha=1).fit(design_matrix_5,yi_5)\n",
    "        h_5 = r.predict(design_matrix_5)\n",
    "        #print h_5\n",
    "        plt.plot(x_5, h_5, label=\"lamda = \"+ str(lamda_5[j]) + \", d= \" + str(d[i]));\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)\n",
    "\n",
    "Kao u zadataku 3b, razdvojite primjere na skup za učenje i skup za ispitivanje u omjeru 1:1. Prikažite krivulje logaritama pogreške učenja i ispitne pogreške u ovisnosti za model $\\mathcal{H}_{d=20,\\lambda}$, podešavajući faktor regularizacije $\\lambda$ u rasponu $\\lambda\\in\\{0,1,\\dots,50\\}$.\n",
    "\n",
    "**Q:** Kojoj strani na grafikonu odgovara područje prenaučenosti, a kojoj podnaučenosti? Zašto?\n",
    "\n",
    "**Q:** Koju biste vrijednosti za $\\lambda$ izabrali na temelju ovih grafikona i zašto?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "x_train_5, x_test_5, y_train_5, y_test_5 = tts(x_5, yi_5, test_size=0.5)\n",
    "error_train_5 = []\n",
    "error_test_5 = []\n",
    "d = 20\n",
    "lamda_5b = range(0,50)\n",
    "\n",
    "for l in lamda_5b:\n",
    "    design_matrix_train_5 = pf(d).fit_transform(x_train_5.reshape(-1,1))\n",
    "    design_matrix_test_5 = pf(d).fit_transform(x_test_5.reshape(-1,1))\n",
    "    w_train_5 = np.dot(np.dot(np.linalg.inv(np.dot(design_matrix_train_5.T, design_matrix_train_5) + np.dot(l, np.eye(d+1))),design_matrix_train_5.T), y_train_5)\n",
    "    h_train_5 = np.dot(design_matrix_train_5, w_train_5)\n",
    "    h_test_5 = np.dot(design_matrix_test_5, w_train_5)\n",
    "    error_train_5.insert(l, np.log(MSE(y_train_5, h_train_5)))\n",
    "    error_test_5.insert(l, np.log(MSE(y_test_5, h_test_5)))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lamda_5b, error_train_5, lamda_5b, error_test_5)\n",
    "plt.legend(['Train error', 'Test error'], loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. L1-regularizacija i L2-regularizacija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Svrha regularizacije jest potiskivanje težina modela $\\mathbf{w}$ prema nuli, kako bi model bio što jednostavniji. Složenost modela može se okarakterizirati normom pripadnog vektora težina $\\mathbf{w}$, i to tipično L2-normom ili L1-normom. Za jednom trenirani model možemo izračunati i broj ne-nul značajki, ili L0-normu, pomoću sljedeće funkcije:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonzeroes(coef, tol=1e-6): \n",
    "    return len(coef) - len(coef[np.isclose(0, coef, atol=tol)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Za ovaj zadatak upotrijebite skup za učenje i skup za testiranje iz zadatka 3b. Trenirajte modele **L2-regularizirane** polinomijalne regresije stupnja $d=20$, mijenjajući hiperparametar $\\lambda$ u rasponu $\\{1,2,\\dots,100\\}$. Za svaki od treniranih modela izračunajte L{0,1,2}-norme vektora težina $\\mathbf{w}$ te ih prikažite kao funkciju od $\\lambda$.\n",
    "\n",
    "**Q:** Objasnite oblik obiju krivulja. Hoće li krivulja za $\\|\\mathbf{w}\\|_2$ doseći nulu? Zašto? Je li to problem? Zašto?\n",
    "\n",
    "**Q:** Za $\\lambda=100$, koliki je postotak težina modela jednak nuli, odnosno koliko je model rijedak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Vaš kôd ovdje\n",
    "l0 = []; l1 = []; l2 = [];\n",
    "d_6 = 20\n",
    "lamda_6 = range(1,100)\n",
    "x_train_6, x_test_6, y_train_6, y_test_6 = tts(x_6, y_6, test_size=0.5)\n",
    "design_matrix_6 = pf(d_6).fit_transform(x_train_6.reshape(-1,1))\n",
    "\n",
    "L1 = lambda tezine: sum(abs(tezine))\n",
    "L2 = lambda tezine: math.sqrt(np.dot(tezine.T, tezine))\n",
    "for l in lamda_6:\n",
    "    r = Ridge(alpha=1)\n",
    "    r.fit(design_matrix_6, y_train_6)\n",
    "    \n",
    "    #w_reg_6 = np.dot(np.dot(np.linalg.inv(np.dot(design_matrix_6.T, design_matrix_6) + np.dot(l, np.eye(d_6+1))),design_matrix_6.T), y_train_6)\n",
    "    l0.append(nonzeroes(r.coef_))\n",
    "    l1.append(L1(r.coef_))\n",
    "    l2.append(L2(r.coef_))\n",
    "plt.figure()\n",
    "plot(lamda_6, l0, lamda_6, l1, lamda_6, l2)\n",
    "legend(['L0', 'L1', 'L2'], loc = 'upper right')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glavna prednost L1-regularizirane regresije (ili *LASSO regression*) nad L2-regulariziranom regresijom jest u tome što L1-regularizirana regresija rezultira **rijetkim modelima** (engl. *sparse models*), odnosno modelima kod kojih su mnoge težine pritegnute na nulu. Pokažite da je to doista tako, ponovivši gornji eksperiment s **L1-regulariziranom** regresijom, implementiranom u klasi  [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) u modulu [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "from sklearn.linear_model import Lasso\n",
    "l0 = []; l1 = []; l2 = [];\n",
    "\n",
    "for l in lamda_6:\n",
    "    lasso = Lasso(alpha=l).fit(design_matrix_6, y_train_6)\n",
    "    w_6b = lasso.coef_\n",
    "    \n",
    "    l0.append(nonzeroes(w_6b))\n",
    "    l1.append(L1(w_6b))\n",
    "    l2.append(L2(w_6b))\n",
    "    \n",
    "plot(lamda_6, l0, lamda_6, l1, lamda_6, l2)\n",
    "legend(['L0', 'L1', 'L2'], loc = 'upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Značajke različitih skala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Često se u praksi možemo susreti sa podatcima u kojima sve značajke nisu jednakih magnituda. Primjer jednog takvog skupa je regresijski skup podataka `grades` u kojem se predviđa prosjek ocjena studenta na studiju (1--5) na temelju dvije značajke: bodova na prijamnom ispitu (1--3000) i prosjeka ocjena u srednjoj školi. Prosjek ocjena na studiju izračunat je kao težinska suma ove dvije značajke uz dodani šum.\n",
    "\n",
    "Koristite sljedeći kôd kako biste generirali ovaj skup podataka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_points = 500\n",
    "np.random.seed(69)\n",
    "\n",
    "# Generiraj podatke o bodovima na prijamnom ispitu koristeći normalnu razdiobu i ograniči ih na interval [1, 3000].\n",
    "exam_score = np.random.normal(loc=1500.0, scale = 500.0, size = n_data_points) \n",
    "exam_score = np.round(exam_score)\n",
    "exam_score[exam_score > 3000] = 3000\n",
    "exam_score[exam_score < 0] = 0\n",
    "\n",
    "# Generiraj podatke o ocjenama iz srednje škole koristeći normalnu razdiobu i ograniči ih na interval [1, 5].\n",
    "grade_in_highschool = np.random.normal(loc=3, scale = 2.0, size = n_data_points)\n",
    "grade_in_highschool[grade_in_highschool > 5] = 5\n",
    "grade_in_highschool[grade_in_highschool < 1] = 1\n",
    "\n",
    "# Matrica dizajna.\n",
    "grades_X = np.array([exam_score,grade_in_highschool]).T\n",
    "\n",
    "# Završno, generiraj izlazne vrijednosti.\n",
    "rand_noise = np.random.normal(loc=0.0, scale = 0.5, size = n_data_points)\n",
    "exam_influence = 0.9\n",
    "grades_y = ((exam_score / 3000.0) * (exam_influence) + (grade_in_highschool / 5.0) \\\n",
    "            * (1.0 - exam_influence)) * 5.0 + rand_noise\n",
    "grades_y[grades_y < 1] = 1\n",
    "grades_y[grades_y > 5] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iscrtajte ovisnost ciljne vrijednosti (y-os) o prvoj i o drugoj značajki (x-os). Iscrtajte dva odvojena grafa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "plt.figure()\n",
    "plt.scatter(exam_score, grades_y ,color='g')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(grade_in_highschool, grades_y ,color='r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naučite model L2-regularizirane regresije ($\\lambda = 0.01$), na podacima `grades_X` i `grades_y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "lamda_7 = 0.01\n",
    "\n",
    "w_7 = Ridge(alpha=lamda_7).fit(grades_X, grades_y)\n",
    "print w_7.coef_\n",
    "\n",
    "wo_l2 = w_7.intercept_\n",
    "weights = []\n",
    "weights.append(wo_l2)\n",
    "for j in range(0, len(w_7.coef_[0:])):\n",
    "    weights.append(w_7.coef_[j])\n",
    "\n",
    "print weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada ponovite gornji eksperiment, ali prvo skalirajte podatke `grades_X` i `grades_y` i spremite ih u varijable `grades_X_fixed` i `grades_y_fixed`. Za tu svrhu, koristite [`StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Vaš kôd ovdje\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(grades_X)\n",
    "grades_X_fixed = scaler.transform(grades_X)\n",
    "scaler.fit(grades_y.reshape(-1,1))\n",
    "grades_y_fixed = scaler.transform(grades_y.reshape(-1,1))\n",
    "\n",
    "w_7c = Ridge(alpha=lamda_7)\n",
    "w_7c.fit(grades_X_fixed, grades_y_fixed)\n",
    "print w_7c.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Gledajući grafikone iz podzadatka (a), koja značajka bi trebala imati veću magnitudu, odnosno važnost pri predikciji prosjeka na studiju? Odgovaraju li težine Vašoj intuiciji? Objasnite.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Multikolinearnost i kondicija matrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Izradite skup podataka `grades_X_fixed_colinear` tako što ćete u skupu `grades_X_fixed` iz\n",
    "zadatka 7b duplicirati zadnji stupac (ocjenu iz srednje škole). Time smo efektivno uveli savršenu multikolinearnost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "grades_X_fixed_colinear = np.hstack((grades_X_fixed, np.tile(grades_X_fixed[:, [-1]], 1)))\n",
    "print grades_X_fixed_colinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponovno, naučite na ovom skupu L2-regularizirani model regresije ($\\lambda = 0.01$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "w_8 = Ridge(alpha=lamda_7).fit(grades_X_fixed_colinear, grades_y_fixed)\n",
    "print(w_8.coef_)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**Q:** Usporedite iznose težina s onima koje ste dobili u zadatku *7b*. Što se dogodilo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slučajno uzorkujte 50% elemenata iz skupa `grades_X_fixed_colinear` i naučite dva modela L2-regularizirane regresije, jedan s $\\lambda=0.01$, a jedan s $\\lambda=1000$. Ponovite ovaj pokus 10 puta (svaki put s drugim podskupom od 50% elemenata).  Za svaki model, ispišite dobiveni vektor težina u svih 10 ponavljanja te ispišite standardnu devijaciju vrijednosti svake od težina (ukupno šest standardnih devijacija, svaka dobivena nad 10 vrijednosti)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "lamda_1 = 0.01\n",
    "lamda_2 = 1000\n",
    "br = len(grades_X_fixed_colinear)\n",
    "vector_1 = []\n",
    "vector_2 = []\n",
    "for i in range(0,10):\n",
    "    index_list = np.random.choice(np.arange(br), int(br/2), replace=False)\n",
    "    X1_train8 = grades_X_fixed_colinear[index_list]\n",
    "    y1_train8 = grades_y_fixed[index_list]\n",
    "    w_81 = Ridge(alpha=lamda_1).fit(X1_train8, y1_train8)\n",
    "    vector_1.append(w_81.coef_)\n",
    "    print(\"Pokusaj broj: \", i, \": Tezine za lamda = \", lamda_1, \" iznose: \", w_81.coef_)\n",
    "    w_82 = Ridge(alpha=lamda_2).fit(X1_train8, y1_train8)\n",
    "    vector_2.append(w_82.coef_)\n",
    "    print(\"Pokusaj broj: \", i, \": Tezine za lamda = \", lamda_2, \" iznose: \", w_82.coef_)\n",
    "\n",
    "mean_1 = np.mean(vector_1, axis = 0)\n",
    "mean_2 = np.mean(vector_2, axis = 0)\n",
    "sd_1 = np.std(vector_1, axis = 0)\n",
    "sd_2 = np.std(vector_2, axis = 0)\n",
    "\n",
    "print(\"Mean za lamda = \", lamda_1, \"je: \", mean_1)\n",
    "print(\"Standardna devijacija za lamda = \", lamda_1, \"je: \", sd_1)\n",
    "print(\"Mean za lamda = \", lamda_2, \"je: \", mean_2)\n",
    "print(\"Standardna devijacija za lamda = \", lamda_2, \"je: \", sd_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako regularizacija utječe na stabilnost težina?  \n",
    "**Q:** Jesu li koeficijenti jednakih magnituda kao u prethodnom pokusu? Objasnite zašto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koristeći [`numpy.linalg.cond`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.cond.html) izračunajte kondicijski broj matrice $\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi}+\\lambda\\mathbf{I}$, gdje je $\\mathbf{\\Phi}$ matrica dizajna (`grades_fixed_X_colinear`). Ponovite i za $\\lambda=0.01$ i za $\\lambda=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje\n",
    "temp = np.dot(grades_X_fixed_colinear.transpose(), grades_X_fixed_colinear)\n",
    "temp2 = temp + 0.01*np.identity(np.shape(temp)[0])\n",
    "print(\"kondicijski broj za lamda = 0.01: \", lin.cond(temp2))\n",
    "temp3 = temp + 10*np.identity(np.shape(temp)[0])\n",
    "print(\"kondicijski broj za lamda = 10: \", lin.cond(temp3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako regularizacija utječe na kondicijski broj matrice $\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi}+\\lambda\\mathbf{I}$?  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "SageMath (stable)",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
